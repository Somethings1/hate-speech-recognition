{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0UeHCx7aNvW",
        "outputId": "bdcf3eca-3230-4370-c1b2-e3a09e06c7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.46.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "âœ… Gemini Client successfully initialized with API key.\n",
            "Mounted at /content/drive\n",
            "Setup complete. Libraries installed and imports ready.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install necessary libraries\n",
        "# 1. Install necessary libraries\n",
        "!pip install pandas pydantic google-genai requests\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import requests # To fetch TikTok video context\n",
        "from typing import List, Dict, Optional\n",
        "from pydantic import BaseModel, Field # For structured output schema\n",
        "import time # For API call rate limiting\n",
        "from google import genai # Using this import as shown in your traceback\n",
        "\n",
        "# --- LLM Response Schema (CRUCIAL for structured output) ---\n",
        "class CommentClassification(BaseModel):\n",
        "    \"\"\"Schema for a single comment's classification result.\"\"\"\n",
        "    comment_id: str = Field(..., description=\"The original comment_id from the input.\")\n",
        "    classification: str = Field(..., description=\"The classification: 'HATE_SPEECH' or 'NOT_HATE_SPEECH'.\")\n",
        "    confidence: float = Field(..., description=\"A confidence score from 0.0 to 1.0.\")\n",
        "    justification: str = Field(..., description=\"A brief Vietnamese reason for the decision, citing the comment or video context.\")\n",
        "\n",
        "class ClassificationList(BaseModel):\n",
        "    \"\"\"The full list of classification results returned by the LLM.\"\"\"\n",
        "    results: List[CommentClassification]\n",
        "\n",
        "# --- LLM API Setup (CORRECTED) ---\n",
        "# NOTE: This code is designed to run in Google Colab.\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # 1. Click the \"Key\" icon (ðŸ”‘) on the left panel of Colab.\n",
        "    # 2. Add a new secret.\n",
        "    # 3. Name the secret: GEMINI_API_KEY\n",
        "    # 4. Paste your API key as the value.\n",
        "\n",
        "    API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    if not API_KEY:\n",
        "        raise ValueError(\"API key not found in Colab Secrets. Please add it.\")\n",
        "\n",
        "    # Pass the API key DIRECTLY to the Client constructor\n",
        "    client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "    MODEL_NAME = \"gemini-2.5-flash\"\n",
        "    print(\"âœ… Gemini Client successfully initialized with API key.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Not in Colab. Attempting to fall back to environment variables...\")\n",
        "    API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "    if not API_KEY:\n",
        "        print(\"!!! ERROR: GEMINI_API_KEY environment variable not set.\")\n",
        "    else:\n",
        "        # Pass the API key DIRECTLY to the Client constructor\n",
        "        client = genai.Client(api_key=API_KEY)\n",
        "        MODEL_NAME = \"gemini-2.5-flash\"\n",
        "        print(\"âœ… Gemini Client successfully configured from environment variable.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"!!! Error initializing Gemini Client: {e}\")\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Setup complete. Libraries installed and imports ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Preprocessed_Comments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pfVMez7bFZU",
        "outputId": "31d6dad4-666e-4a14-b446-29eb29adc9a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Preprocessed_Comments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TikTok Context Fetching (Using Simplified, Robust Pattern) ---\n",
        "\n",
        "VIDEO_CONTEXT_CACHE = {}\n",
        "\n",
        "def get_video_context(video_id: str, retries: int = 3, delay: float = 0.5) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Fetches the video caption/hashtags using the simple API endpoint.\n",
        "    This is based on your provided, more stable function.\n",
        "    \"\"\"\n",
        "    if video_id in VIDEO_CONTEXT_CACHE:\n",
        "        return VIDEO_CONTEXT_CACHE[video_id]\n",
        "\n",
        "    # Using the simplified endpoint and params you provided\n",
        "    API_ENDPOINT = \"https://www.tiktok.com/api/comment/list/\"\n",
        "    # We only need 1 comment to get the share_info\n",
        "    PARAMS = {\"aid\": \"1988\", \"aweme_id\": video_id, \"count\": 1, \"cursor\": 0}\n",
        "    HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "\n",
        "    # --- Retry and Delay Logic ---\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Add delay BEFORE the request to respect the API endpoint\n",
        "            print(f\"  > Attempt {attempt+1}/{retries} for {video_id}. Waiting {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "            response = requests.get(API_ENDPOINT, params=PARAMS, headers=HEADERS, timeout=10)\n",
        "            response.raise_for_status() # Raise an exception for 4xx/5xx status codes\n",
        "            data = response.json()\n",
        "\n",
        "            if 'comments' in data and data['comments']:\n",
        "                # Extract context from the 'share_info' of the first comment\n",
        "                title = data['comments'][0]['share_info']['title']\n",
        "                VIDEO_CONTEXT_CACHE[video_id] = title.strip()\n",
        "                return title.strip()\n",
        "\n",
        "            # If 'comments' key exists but is empty\n",
        "            VIDEO_CONTEXT_CACHE[video_id] = \"Context Not Found (Empty Comments).\"\n",
        "            return \"Context Not Found (Empty Comments).\"\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  > TikTok API Error (Attempt {attempt+1}): {e}\")\n",
        "            if attempt == retries - 1: # Last attempt failed\n",
        "                VIDEO_CONTEXT_CACHE[video_id] = \"API Failure.\"\n",
        "                return \"API Failure.\"\n",
        "            # Wait longer before retrying\n",
        "            time.sleep(1) # Extra 1s wait on failure\n",
        "\n",
        "    # Fallback in case loop finishes unexpectedly\n",
        "    return \"API Failure.\"\n",
        "\n",
        "\n",
        "## Function to Load Data (Using 4-digit zero-padding)\n",
        "def load_and_merge_comments(file_prefix: str = 'clean_comments_batch_', num_files: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"Loads a few comment files and merges them into a single DataFrame.\"\"\"\n",
        "    all_data = []\n",
        "    print(f\"Loading files with prefix: {file_prefix}XXXX.csv\")\n",
        "    for i in range(1, num_files + 1):\n",
        "        try:\n",
        "            # Use 4-digit zero-padding (e.g., 0001, 0002)\n",
        "            file_name = f\"{file_prefix}{i:04d}.csv\"\n",
        "            df = pd.read_csv(file_name)\n",
        "            all_data.append(df)\n",
        "            print(f\"Loaded {file_name}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {file_name}. Stopping data loading.\")\n",
        "            break\n",
        "\n",
        "    if all_data:\n",
        "        merged_df = pd.concat(all_data, ignore_index=True)\n",
        "        merged_df['video_id'] = merged_df['video_id'].astype(str) # 'aweme_id'\n",
        "        return merged_df.drop_duplicates(subset=['comment_id'])\n",
        "    return pd.DataFrame()\n",
        "\n",
        "## Load Data (change num_files as needed)\n",
        "df_comments = load_and_merge_comments(file_prefix='clean_comments_batch_', num_files=3)\n",
        "print(f\"\\nTotal unique comments loaded: {len(df_comments)}\")\n",
        "\n",
        "## Add Context to Comments and Create JSON Payload (Using 'clean_text')\n",
        "def enrich_comments_with_context(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Fetches video context and creates the structured payload for the prompt.\"\"\"\n",
        "    enriched_data = []\n",
        "\n",
        "    unique_video_ids = df['video_id'].unique()\n",
        "    print(f\"\\nFetching context for {len(unique_video_ids)} unique videos...\")\n",
        "\n",
        "    for i, vid in enumerate(unique_video_ids):\n",
        "        print(f\"Processing Video {i+1}/{len(unique_video_ids)} (ID: {vid})\")\n",
        "        # The delay and retry logic is handled inside get_video_context()\n",
        "        get_video_context(vid) # Populates the cache\n",
        "\n",
        "    print(\"\\nContext fetching complete. Starting comment structurization...\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        vid = row['video_id']\n",
        "        video_context_combined = VIDEO_CONTEXT_CACHE.get(vid, \"Context Not Found.\")\n",
        "\n",
        "        # Check if 'clean_text' column exists, fallback to 'comment_text' if not\n",
        "        comment_content = row['clean_text'] if 'clean_text' in row and pd.notna(row['clean_text']) else row['comment_text']\n",
        "\n",
        "        comment_data = {\n",
        "            \"comment_id\": str(row['comment_id']),\n",
        "            \"video_context\": video_context_combined,\n",
        "            \"commenter_username\": row['username'],\n",
        "            # --- Using 'clean_text' (with fallback) ---\n",
        "            \"comment_content\": comment_content,\n",
        "        }\n",
        "\n",
        "        row_with_context = row.to_dict()\n",
        "        row_with_context['video_context'] = video_context_combined\n",
        "        row_with_context['comment_json'] = json.dumps(comment_data, ensure_ascii=False)\n",
        "        enriched_data.append(row_with_context)\n",
        "\n",
        "    return pd.DataFrame(enriched_data)\n",
        "\n",
        "df_enriched = enrich_comments_with_context(df_comments)\n",
        "if not df_enriched.empty:\n",
        "    print(f\"\\nExample of structured object with context:\\n{df_enriched['comment_json'].iloc[0]}\")\n",
        "else:\n",
        "    print(\"\\nNo data loaded or enriched. Check CSV file names and paths.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HTo-KXOdPfc",
        "outputId": "a3f7cdde-a1b4-4500-ca5c-a0a22e1bb7cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading files with prefix: clean_comments_batch_XXXX.csv\n",
            "Loaded clean_comments_batch_0001.csv\n",
            "Loaded clean_comments_batch_0002.csv\n",
            "Loaded clean_comments_batch_0003.csv\n",
            "\n",
            "Total unique comments loaded: 3000\n",
            "\n",
            "Fetching context for 58 unique videos...\n",
            "Processing Video 1/58 (ID: 6989137370817842459)\n",
            "  > Attempt 1/3 for 6989137370817842459. Waiting 0.5s...\n",
            "Processing Video 2/58 (ID: 6994046144053660930)\n",
            "  > Attempt 1/3 for 6994046144053660930. Waiting 0.5s...\n",
            "Processing Video 3/58 (ID: 7013362152782171419)\n",
            "  > Attempt 1/3 for 7013362152782171419. Waiting 0.5s...\n",
            "Processing Video 4/58 (ID: 7040406003837373698)\n",
            "  > Attempt 1/3 for 7040406003837373698. Waiting 0.5s...\n",
            "Processing Video 5/58 (ID: 7041523740936572162)\n",
            "  > Attempt 1/3 for 7041523740936572162. Waiting 0.5s...\n",
            "Processing Video 6/58 (ID: 7044415240049528065)\n",
            "  > Attempt 1/3 for 7044415240049528065. Waiting 0.5s...\n",
            "Processing Video 7/58 (ID: 7067497026837105946)\n",
            "  > Attempt 1/3 for 7067497026837105946. Waiting 0.5s...\n",
            "Processing Video 8/58 (ID: 7079327904911412507)\n",
            "  > Attempt 1/3 for 7079327904911412507. Waiting 0.5s...\n",
            "Processing Video 9/58 (ID: 7121668783567113498)\n",
            "  > Attempt 1/3 for 7121668783567113498. Waiting 0.5s...\n",
            "Processing Video 10/58 (ID: 7122090630163041563)\n",
            "  > Attempt 1/3 for 7122090630163041563. Waiting 0.5s...\n",
            "Processing Video 11/58 (ID: 7139855079187778843)\n",
            "  > Attempt 1/3 for 7139855079187778843. Waiting 0.5s...\n",
            "Processing Video 12/58 (ID: 7145739858370120987)\n",
            "  > Attempt 1/3 for 7145739858370120987. Waiting 0.5s...\n",
            "Processing Video 13/58 (ID: 7151196519641763098)\n",
            "  > Attempt 1/3 for 7151196519641763098. Waiting 0.5s...\n",
            "Processing Video 14/58 (ID: 7158060512800935174)\n",
            "  > Attempt 1/3 for 7158060512800935174. Waiting 0.5s...\n",
            "Processing Video 15/58 (ID: 7173113067818028331)\n",
            "  > Attempt 1/3 for 7173113067818028331. Waiting 0.5s...\n",
            "Processing Video 16/58 (ID: 7173113792354651438)\n",
            "  > Attempt 1/3 for 7173113792354651438. Waiting 0.5s...\n",
            "Processing Video 17/58 (ID: 7174023771693616427)\n",
            "  > Attempt 1/3 for 7174023771693616427. Waiting 0.5s...\n",
            "Processing Video 18/58 (ID: 7186146452840680746)\n",
            "  > Attempt 1/3 for 7186146452840680746. Waiting 0.5s...\n",
            "Processing Video 19/58 (ID: 7212971585190333723)\n",
            "  > Attempt 1/3 for 7212971585190333723. Waiting 0.5s...\n",
            "Processing Video 20/58 (ID: 7219723328599100714)\n",
            "  > Attempt 1/3 for 7219723328599100714. Waiting 0.5s...\n",
            "Processing Video 21/58 (ID: 7223239299494464774)\n",
            "  > Attempt 1/3 for 7223239299494464774. Waiting 0.5s...\n",
            "Processing Video 22/58 (ID: 7225225944699604230)\n",
            "  > Attempt 1/3 for 7225225944699604230. Waiting 0.5s...\n",
            "Processing Video 23/58 (ID: 7225835952240119086)\n",
            "  > Attempt 1/3 for 7225835952240119086. Waiting 0.5s...\n",
            "Processing Video 24/58 (ID: 7228588029714156805)\n",
            "  > Attempt 1/3 for 7228588029714156805. Waiting 0.5s...\n",
            "Processing Video 25/58 (ID: 7229556830987488518)\n",
            "  > Attempt 1/3 for 7229556830987488518. Waiting 0.5s...\n",
            "Processing Video 26/58 (ID: 7229906581297564933)\n",
            "  > Attempt 1/3 for 7229906581297564933. Waiting 0.5s...\n",
            "Processing Video 27/58 (ID: 7230671160520789253)\n",
            "  > Attempt 1/3 for 7230671160520789253. Waiting 0.5s...\n",
            "Processing Video 28/58 (ID: 7234481706240101658)\n",
            "  > Attempt 1/3 for 7234481706240101658. Waiting 0.5s...\n",
            "Processing Video 29/58 (ID: 7240264530637376795)\n",
            "  > Attempt 1/3 for 7240264530637376795. Waiting 0.5s...\n",
            "Processing Video 30/58 (ID: 7241051633092791578)\n",
            "  > Attempt 1/3 for 7241051633092791578. Waiting 0.5s...\n",
            "Processing Video 31/58 (ID: 7242132747718839578)\n",
            "  > Attempt 1/3 for 7242132747718839578. Waiting 0.5s...\n",
            "Processing Video 32/58 (ID: 7242578411003071749)\n",
            "  > Attempt 1/3 for 7242578411003071749. Waiting 0.5s...\n",
            "Processing Video 33/58 (ID: 7242809059295055110)\n",
            "  > Attempt 1/3 for 7242809059295055110. Waiting 0.5s...\n",
            "Processing Video 34/58 (ID: 7243004421699931398)\n",
            "  > Attempt 1/3 for 7243004421699931398. Waiting 0.5s...\n",
            "Processing Video 35/58 (ID: 7243010977145376005)\n",
            "  > Attempt 1/3 for 7243010977145376005. Waiting 0.5s...\n",
            "Processing Video 36/58 (ID: 7244457891011333381)\n",
            "  > Attempt 1/3 for 7244457891011333381. Waiting 0.5s...\n",
            "Processing Video 37/58 (ID: 7247855742076603654)\n",
            "  > Attempt 1/3 for 7247855742076603654. Waiting 0.5s...\n",
            "Processing Video 38/58 (ID: 7248434734131383558)\n",
            "  > Attempt 1/3 for 7248434734131383558. Waiting 0.5s...\n",
            "Processing Video 39/58 (ID: 7249384346656853274)\n",
            "  > Attempt 1/3 for 7249384346656853274. Waiting 0.5s...\n",
            "Processing Video 40/58 (ID: 7251555866371951877)\n",
            "  > Attempt 1/3 for 7251555866371951877. Waiting 0.5s...\n",
            "Processing Video 41/58 (ID: 7252985880061824262)\n",
            "  > Attempt 1/3 for 7252985880061824262. Waiting 0.5s...\n",
            "Processing Video 42/58 (ID: 7253007305447361797)\n",
            "  > Attempt 1/3 for 7253007305447361797. Waiting 0.5s...\n",
            "Processing Video 43/58 (ID: 7255633064406027526)\n",
            "  > Attempt 1/3 for 7255633064406027526. Waiting 0.5s...\n",
            "Processing Video 44/58 (ID: 7258616590357351685)\n",
            "  > Attempt 1/3 for 7258616590357351685. Waiting 0.5s...\n",
            "Processing Video 45/58 (ID: 7263711540572917025)\n",
            "  > Attempt 1/3 for 7263711540572917025. Waiting 0.5s...\n",
            "Processing Video 46/58 (ID: 7264586731633020193)\n",
            "  > Attempt 1/3 for 7264586731633020193. Waiting 0.5s...\n",
            "Processing Video 47/58 (ID: 7264926090722839809)\n",
            "  > Attempt 1/3 for 7264926090722839809. Waiting 0.5s...\n",
            "Processing Video 48/58 (ID: 7266094413074402562)\n",
            "  > Attempt 1/3 for 7266094413074402562. Waiting 0.5s...\n",
            "Processing Video 49/58 (ID: 7270385183482727685)\n",
            "  > Attempt 1/3 for 7270385183482727685. Waiting 0.5s...\n",
            "Processing Video 50/58 (ID: 7271978389601324293)\n",
            "  > Attempt 1/3 for 7271978389601324293. Waiting 0.5s...\n",
            "Processing Video 51/58 (ID: 7272224684429561130)\n",
            "  > Attempt 1/3 for 7272224684429561130. Waiting 0.5s...\n",
            "Processing Video 52/58 (ID: 7273867547613629698)\n",
            "  > Attempt 1/3 for 7273867547613629698. Waiting 0.5s...\n",
            "Processing Video 53/58 (ID: 7275218423091563778)\n",
            "  > Attempt 1/3 for 7275218423091563778. Waiting 0.5s...\n",
            "Processing Video 54/58 (ID: 7275242958876724488)\n",
            "  > Attempt 1/3 for 7275242958876724488. Waiting 0.5s...\n",
            "Processing Video 55/58 (ID: 7275756556363287816)\n",
            "  > Attempt 1/3 for 7275756556363287816. Waiting 0.5s...\n",
            "Processing Video 56/58 (ID: 7275854462730685739)\n",
            "  > Attempt 1/3 for 7275854462730685739. Waiting 0.5s...\n",
            "Processing Video 57/58 (ID: 7277419359381884161)\n",
            "  > Attempt 1/3 for 7277419359381884161. Waiting 0.5s...\n",
            "Processing Video 58/58 (ID: 7278156131120270635)\n",
            "  > Attempt 1/3 for 7278156131120270635. Waiting 0.5s...\n",
            "\n",
            "Context fetching complete. Starting comment structurization...\n",
            "\n",
            "Example of structured object with context:\n",
            "{\"comment_id\": \"7461965801051144968\", \"video_context\": \"Äu trend phiÃªn báº£n má»›i cáº¥n thai ðŸ¥²\", \"commenter_username\": \"NgÃ´ ChÃ¢u Anh\", \"comment_content\": \"2025 cÃ²n xem ko\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Actual LLM API Call with Structured Output\n",
        "def call_gemini_api(prompt: str) -> List[Dict]:\n",
        "    \"\"\"Calls the Gemini API with structured output configuration.\"\"\"\n",
        "    global client\n",
        "\n",
        "    SYSTEM_INSTRUCTION = \"\"\"\n",
        "    You are an expert Vietnamese content moderator. Your task is to classify TikTok comments as hate speech or not.\n",
        "    Hate speech is defined as any comment that **insults, degrades, or attacks any individual or social group**.\n",
        "\n",
        "    Classification Rules:\n",
        "    1. **Context is Key:** Use the 'video_context' to understand the comment's intent.\n",
        "    2. **Hate Speech:** Classify as **'HATE_SPEECH'** if the comment contains insults, degradation, or attacks.\n",
        "    3. **Ambiguity:** If the comment contains Vietnamese slang, inside jokes, or is out of context, and you cannot confidently classify it, assign a **LOW confidence score (e.g., 0.1 to 0.4)**.\n",
        "    4. **Output:** Your entire response MUST be a single JSON object that strictly adheres to the provided schema (a list of results).\n",
        "    \"\"\"\n",
        "\n",
        "    generation_config = genai.types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=ClassificationList,\n",
        "        system_instruction=SYSTEM_INSTRUCTION\n",
        "    )\n",
        "\n",
        "    contents = [\n",
        "        {\"role\": \"user\", \"parts\": [{\"text\": prompt}]}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_NAME,\n",
        "            contents=contents,\n",
        "            config=generation_config\n",
        "        )\n",
        "        response_json = json.loads(response.text)\n",
        "        return response_json.get('results', [])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! API/JSON Error during call: {e}\")\n",
        "        return []\n",
        "\n",
        "## Prompt Generation and Batching Experiment\n",
        "BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
        "MAX_COMMENTS = sum(BATCH_SIZES)\n",
        "\n",
        "df_test = df_enriched.head(MAX_COMMENTS).copy()\n",
        "print(f\"Using {len(df_test)} comments for the batching experiment.\")\n",
        "\n",
        "experiment_results = []\n",
        "start_index = 0\n",
        "\n",
        "print(\"\\n--- Starting LLM Batching Experiment ---\")\n",
        "\n",
        "if df_test.empty:\n",
        "    print(\"No data to test. Stopping experiment.\")\n",
        "else:\n",
        "    for batch_size in BATCH_SIZES:\n",
        "        print(f\"\\nProcessing batch size: {batch_size}...\")\n",
        "\n",
        "        batch_data = df_test.iloc[start_index:start_index + batch_size]\n",
        "        if batch_data.empty:\n",
        "            print(\"No more comments left. Stopping.\")\n",
        "            break\n",
        "\n",
        "        comment_json_list = ',\\n'.join(batch_data['comment_json'].tolist())\n",
        "\n",
        "        user_prompt = f\"\"\"\n",
        "        Please classify the following {len(batch_data)} TikTok comments based on the provided rules.\n",
        "\n",
        "        Input Comments (List of JSON Objects):\n",
        "        ---START_OF_INPUT---\n",
        "        [\n",
        "        {comment_json_list}\n",
        "        ]\n",
        "        ---END_OF_INPUT---\n",
        "        \"\"\"\n",
        "\n",
        "        current_batch_ids = batch_data['comment_id'].tolist()\n",
        "\n",
        "        llm_results = call_gemini_api(user_prompt)\n",
        "\n",
        "        if len(llm_results) != len(current_batch_ids):\n",
        "             print(f\"!!! WARNING: LLM returned {len(llm_results)} results, expected {len(current_batch_ids)}. This batch size may be unstable.\")\n",
        "\n",
        "        for result in llm_results:\n",
        "            result['batch_size_tested'] = batch_size\n",
        "\n",
        "        experiment_results.extend(llm_results)\n",
        "\n",
        "        start_index += batch_size\n",
        "        # Add a small delay between batches\n",
        "        time.sleep(1)\n",
        "\n",
        "print(\"\\n--- LLM Experiment Complete ---\")\n",
        "print(f\"Total classification results collected: {len(experiment_results)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SPmXG2wdm-F",
        "outputId": "e4832599-2a5f-4e92-dfaf-5bba18f259b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 1023 comments for the batching experiment.\n",
            "\n",
            "--- Starting LLM Batching Experiment ---\n",
            "\n",
            "Processing batch size: 1...\n",
            "\n",
            "Processing batch size: 2...\n",
            "\n",
            "Processing batch size: 4...\n",
            "\n",
            "Processing batch size: 8...\n",
            "\n",
            "Processing batch size: 16...\n",
            "\n",
            "Processing batch size: 32...\n",
            "\n",
            "Processing batch size: 64...\n",
            "\n",
            "Processing batch size: 128...\n",
            "\n",
            "Processing batch size: 256...\n",
            "\n",
            "Processing batch size: 512...\n",
            "!!! API/JSON Error during call: Expecting value: line 184 column 20 (char 7189)\n",
            "!!! WARNING: LLM returned 0 results, expected 512. This batch size may be unstable.\n",
            "\n",
            "--- LLM Experiment Complete ---\n",
            "Total classification results collected: 511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Process and Merge Results\n",
        "if not experiment_results:\n",
        "    print(\"No results were collected from the LLM. Cannot save files.\")\n",
        "else:\n",
        "    df_results = pd.DataFrame(experiment_results)\n",
        "\n",
        "    df_results = df_results.rename(columns={\n",
        "        'classification': 'llm_hate_speech',\n",
        "        'confidence': 'llm_confidence',\n",
        "        'justification': 'llm_justification'\n",
        "    })\n",
        "\n",
        "    df_results['comment_id'] = df_results['comment_id'].astype(str)\n",
        "    df_enriched['comment_id'] = df_enriched['comment_id'].astype(str)\n",
        "\n",
        "    df_final = pd.merge(\n",
        "        df_enriched.drop(columns=['comment_json']),\n",
        "        df_results,\n",
        "        on='comment_id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal DataFrame structure:\")\n",
        "    print(df_final.columns.tolist())\n",
        "\n",
        "    ## Save Results by Batch Size\n",
        "    print(\"\\n--- Saving Results ---\")\n",
        "    for batch_size in BATCH_SIZES:\n",
        "        df_batch = df_final[df_final['batch_size_tested'] == batch_size].copy()\n",
        "\n",
        "        if not df_batch.empty:\n",
        "            cols_to_keep = [\n",
        "                'comment_id', 'video_id', 'username', 'comment_text',\n",
        "                'digg_count', 'create_time', 'clean_text', 'video_context',\n",
        "                'llm_hate_speech', 'llm_confidence', 'llm_justification',\n",
        "                'batch_size_tested'\n",
        "            ]\n",
        "\n",
        "            df_batch = df_batch[[col for col in cols_to_keep if col in df_batch.columns]]\n",
        "\n",
        "            output_filename = f\"llm_classification_batch_{batch_size}.csv\"\n",
        "            df_batch.to_csv(output_filename, index=False)\n",
        "            print(f\"Saved results for batch size {batch_size} to: {output_filename} ({len(df_batch)} rows)\")\n",
        "\n",
        "    print(\"\\nAll tasks complete. Review the CSV files to determine the optimal batch size.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaBQzrTtheJ-",
        "outputId": "9fd0526e-785b-483b-e6a8-7c4ecce6b7e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final DataFrame structure:\n",
            "['comment_id', 'video_id', 'username', 'comment_text', 'digg_count', 'create_time', 'clean_text', 'video_context', 'llm_hate_speech', 'llm_confidence', 'llm_justification', 'batch_size_tested']\n",
            "\n",
            "--- Saving Results ---\n",
            "Saved results for batch size 1 to: llm_classification_batch_1.csv (1 rows)\n",
            "Saved results for batch size 2 to: llm_classification_batch_2.csv (2 rows)\n",
            "Saved results for batch size 4 to: llm_classification_batch_4.csv (4 rows)\n",
            "Saved results for batch size 8 to: llm_classification_batch_8.csv (8 rows)\n",
            "Saved results for batch size 16 to: llm_classification_batch_16.csv (16 rows)\n",
            "Saved results for batch size 32 to: llm_classification_batch_32.csv (32 rows)\n",
            "Saved results for batch size 64 to: llm_classification_batch_64.csv (64 rows)\n",
            "Saved results for batch size 128 to: llm_classification_batch_128.csv (128 rows)\n",
            "Saved results for batch size 256 to: llm_classification_batch_256.csv (256 rows)\n",
            "\n",
            "All tasks complete. Review the CSV files to determine the optimal batch size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# --- Constants for the Forecast ---\n",
        "BATCH_SIZE = 256\n",
        "NUM_BATCHES_TO_TEST = 10\n",
        "NUM_FILES_TO_LOAD = 3 # Just enough to get ~3000 comments\n",
        "TOTAL_PROJECT_COMMENTS = 131000 # Use your known total for the final forecast\n",
        "\n",
        "token_counts = []\n",
        "\n",
        "print(\"--- Starting Billing Forecast (Optimized) ---\")\n",
        "print(f\"Loading {NUM_FILES_TO_LOAD} files for a sample...\")\n",
        "\n",
        "# 1. Load a sample of comments (using the function from Cell 2)\n",
        "# This re-uses 'load_and_merge_comments' from Cell 2\n",
        "df_sample_comments = load_and_merge_comments(\n",
        "    file_prefix='clean_comments_batch_',\n",
        "    num_files=NUM_FILES_TO_LOAD\n",
        ")\n",
        "\n",
        "if not df_sample_comments.empty and len(df_sample_comments) >= (BATCH_SIZE * NUM_BATCHES_TO_TEST):\n",
        "\n",
        "    # 2. Enrich only the sample comments\n",
        "    # This re-uses 'enrich_comments_with_context' from Cell 2\n",
        "    print(\"\\nEnriching sample comments with video context. This will take a moment...\")\n",
        "    df_enriched_sample = enrich_comments_with_context(df_sample_comments)\n",
        "\n",
        "    # 3. Re-create the System Instruction from Cell 3\n",
        "    SYSTEM_INSTRUCTION = \"\"\"\n",
        "    You are an expert Vietnamese content moderator. Your task is to classify TikTok comments as hate speech or not.\n",
        "    Hate speech is defined as any comment that **insults, degrades, or attacks any individual or social group**.\n",
        "\n",
        "    Classification Rules:\n",
        "    1. **Context is Key:** Use the 'video_context' to understand the comment's intent.\n",
        "    2. **Hate Speech:** Classify as **'HATE_SPEECH'** if the comment contains insults, degradation, or attacks.\n",
        "    3. **Ambiguity:** If the comment contains Vietnamese slang, inside jokes, or is out of context, and you cannot confidently classify it, assign a **LOW confidence score (e.g., 0.1 to 0.4)**.\n",
        "    4. **Output:** Your entire response MUST be a single JSON object that strictly adheres to the provided schema (a list of results).\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nCounting tokens for {NUM_BATCHES_TO_TEST} batches of {BATCH_SIZE} comments...\")\n",
        "    start_index = 0\n",
        "\n",
        "    for i in range(NUM_BATCHES_TO_TEST):\n",
        "        end_index = start_index + BATCH_SIZE\n",
        "        batch_data = df_enriched_sample.iloc[start_index:end_index]\n",
        "\n",
        "        # Get the JSON payload for the batch\n",
        "        comment_json_list = ',\\n'.join(batch_data['comment_json'].tolist())\n",
        "\n",
        "        # Re-create the user prompt\n",
        "        user_prompt = f\"\"\"\n",
        "        Please classify the following {len(batch_data)} TikTok comments based on the provided rules.\n",
        "\n",
        "        Input Comments (List of JSON Objects):\n",
        "        ---START_OF_INPUT---\n",
        "        [\n",
        "        {comment_json_list}\n",
        "        ]\n",
        "        ---END_OF_INPUT---\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the full 'contents' object for the API\n",
        "        contents = [\n",
        "            {\"role\": \"user\", \"parts\": [\n",
        "                {\"text\": SYSTEM_INSTRUCTION},\n",
        "                {\"text\": user_prompt}\n",
        "            ]}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            # 4. Call the count_tokens API\n",
        "            response = client.models.count_tokens(model=MODEL_NAME, contents=contents)\n",
        "            token_count = response.total_tokens\n",
        "            token_counts.append(token_count)\n",
        "            print(f\"  > Batch {i+1} token count: {token_count:,}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  > Error counting tokens for batch {i+1}: {e}\")\n",
        "\n",
        "        start_index = end_index\n",
        "\n",
        "    # 5. Calculate and Report the Forecast\n",
        "    if token_counts:\n",
        "        total_test_tokens = sum(token_counts)\n",
        "        avg_tokens_per_batch = total_test_tokens / len(token_counts)\n",
        "\n",
        "        # Use math.ceil to ensure we process all comments\n",
        "        total_batches_needed = math.ceil(TOTAL_PROJECT_COMMENTS / BATCH_SIZE)\n",
        "\n",
        "        total_estimated_tokens = total_batches_needed * avg_tokens_per_batch\n",
        "\n",
        "        print(\"\\n--- Token Analysis (10 Batches) ---\")\n",
        "        print(f\"Average INPUT tokens per {BATCH_SIZE}-comment batch: {avg_tokens_per_batch:,.0f}\")\n",
        "\n",
        "        print(\"\\n--- Total Project Forecast ({TOTAL_PROJECT_COMMENTS:,} Comments) ---\")\n",
        "        print(f\"Total batches needed (at {BATCH_SIZE}/batch): {total_batches_needed:,}\")\n",
        "        print(f\"Total estimated INPUT tokens for all comments: {total_estimated_tokens:,.0f}\")\n",
        "\n",
        "        print(\"\\n--- Billing Forecast ---\")\n",
        "        print(\"To calculate your cost, use this formula:\")\n",
        "        print(\"  (Total Estimated Tokens / 1,000,000) * $PRICE_PER_1M_INPUT_TOKENS\")\n",
        "        print(f\"\\nExample using '{MODEL_NAME}' pricing ($0.08 per 1M input tokens, <128k context):\")\n",
        "\n",
        "        # Example calculation (price is an assumption)\n",
        "        example_price_per_1m = 0.08\n",
        "        example_cost = (total_estimated_tokens / 1_000_000) * example_price_per_1m\n",
        "\n",
        "        print(f\"  ({total_estimated_tokens:,.0f} / 1,000,000) * ${example_price_per_1m} = ${example_cost:.4f}\")\n",
        "\n",
        "        print(\"\\n*Note: This forecast is for INPUT tokens only. Output tokens are billed separately but are usually much smaller for classification tasks.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No token counts were calculated. Check data loading and API connection.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to load enough data. Needed {BATCH_SIZE * NUM_BATCHES_TO_TEST} comments but only loaded {len(df_sample_comments)}. Check file names or increase 'NUM_FILES_TO_LOAD'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWK0VZOrhnQm",
        "outputId": "36d8f8ff-d3d7-44e6-8917-a568eccb6d2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Billing Forecast (Optimized) ---\n",
            "Loading 3 files for a sample...\n",
            "Loading files with prefix: clean_comments_batch_XXXX.csv\n",
            "Loaded clean_comments_batch_0001.csv\n",
            "Loaded clean_comments_batch_0002.csv\n",
            "Loaded clean_comments_batch_0003.csv\n",
            "\n",
            "Enriching sample comments with video context. This will take a moment...\n",
            "\n",
            "Fetching context for 58 unique videos...\n",
            "Processing Video 1/58 (ID: 6989137370817842459)\n",
            "Processing Video 2/58 (ID: 6994046144053660930)\n",
            "Processing Video 3/58 (ID: 7013362152782171419)\n",
            "Processing Video 4/58 (ID: 7040406003837373698)\n",
            "Processing Video 5/58 (ID: 7041523740936572162)\n",
            "Processing Video 6/58 (ID: 7044415240049528065)\n",
            "Processing Video 7/58 (ID: 7067497026837105946)\n",
            "Processing Video 8/58 (ID: 7079327904911412507)\n",
            "Processing Video 9/58 (ID: 7121668783567113498)\n",
            "Processing Video 10/58 (ID: 7122090630163041563)\n",
            "Processing Video 11/58 (ID: 7139855079187778843)\n",
            "Processing Video 12/58 (ID: 7145739858370120987)\n",
            "Processing Video 13/58 (ID: 7151196519641763098)\n",
            "Processing Video 14/58 (ID: 7158060512800935174)\n",
            "Processing Video 15/58 (ID: 7173113067818028331)\n",
            "Processing Video 16/58 (ID: 7173113792354651438)\n",
            "Processing Video 17/58 (ID: 7174023771693616427)\n",
            "Processing Video 18/58 (ID: 7186146452840680746)\n",
            "Processing Video 19/58 (ID: 7212971585190333723)\n",
            "Processing Video 20/58 (ID: 7219723328599100714)\n",
            "Processing Video 21/58 (ID: 7223239299494464774)\n",
            "Processing Video 22/58 (ID: 7225225944699604230)\n",
            "Processing Video 23/58 (ID: 7225835952240119086)\n",
            "Processing Video 24/58 (ID: 7228588029714156805)\n",
            "Processing Video 25/58 (ID: 7229556830987488518)\n",
            "Processing Video 26/58 (ID: 7229906581297564933)\n",
            "Processing Video 27/58 (ID: 7230671160520789253)\n",
            "Processing Video 28/58 (ID: 7234481706240101658)\n",
            "Processing Video 29/58 (ID: 7240264530637376795)\n",
            "Processing Video 30/58 (ID: 7241051633092791578)\n",
            "Processing Video 31/58 (ID: 7242132747718839578)\n",
            "Processing Video 32/58 (ID: 7242578411003071749)\n",
            "Processing Video 33/58 (ID: 7242809059295055110)\n",
            "Processing Video 34/58 (ID: 7243004421699931398)\n",
            "Processing Video 35/58 (ID: 7243010977145376005)\n",
            "Processing Video 36/58 (ID: 7244457891011333381)\n",
            "Processing Video 37/58 (ID: 7247855742076603654)\n",
            "Processing Video 38/58 (ID: 7248434734131383558)\n",
            "Processing Video 39/58 (ID: 7249384346656853274)\n",
            "Processing Video 40/58 (ID: 7251555866371951877)\n",
            "Processing Video 41/58 (ID: 7252985880061824262)\n",
            "Processing Video 42/58 (ID: 7253007305447361797)\n",
            "Processing Video 43/58 (ID: 7255633064406027526)\n",
            "Processing Video 44/58 (ID: 7258616590357351685)\n",
            "Processing Video 45/58 (ID: 7263711540572917025)\n",
            "Processing Video 46/58 (ID: 7264586731633020193)\n",
            "Processing Video 47/58 (ID: 7264926090722839809)\n",
            "Processing Video 48/58 (ID: 7266094413074402562)\n",
            "Processing Video 49/58 (ID: 7270385183482727685)\n",
            "Processing Video 50/58 (ID: 7271978389601324293)\n",
            "Processing Video 51/58 (ID: 7272224684429561130)\n",
            "Processing Video 52/58 (ID: 7273867547613629698)\n",
            "Processing Video 53/58 (ID: 7275218423091563778)\n",
            "Processing Video 54/58 (ID: 7275242958876724488)\n",
            "Processing Video 55/58 (ID: 7275756556363287816)\n",
            "Processing Video 56/58 (ID: 7275854462730685739)\n",
            "Processing Video 57/58 (ID: 7277419359381884161)\n",
            "Processing Video 58/58 (ID: 7278156131120270635)\n",
            "\n",
            "Context fetching complete. Starting comment structurization...\n",
            "\n",
            "Counting tokens for 10 batches of 256 comments...\n",
            "  > Batch 1 token count: 16,636\n",
            "  > Batch 2 token count: 20,435\n",
            "  > Batch 3 token count: 18,958\n",
            "  > Batch 4 token count: 19,338\n",
            "  > Batch 5 token count: 21,121\n",
            "  > Batch 6 token count: 20,944\n",
            "  > Batch 7 token count: 20,471\n",
            "  > Batch 8 token count: 22,772\n",
            "  > Batch 9 token count: 23,559\n",
            "  > Batch 10 token count: 23,135\n",
            "\n",
            "--- Token Analysis (10 Batches) ---\n",
            "Average INPUT tokens per 256-comment batch: 20,737\n",
            "\n",
            "--- Total Project Forecast ({TOTAL_PROJECT_COMMENTS:,} Comments) ---\n",
            "Total batches needed (at 256/batch): 512\n",
            "Total estimated INPUT tokens for all comments: 10,617,293\n",
            "\n",
            "--- Billing Forecast ---\n",
            "To calculate your cost, use this formula:\n",
            "  (Total Estimated Tokens / 1,000,000) * $PRICE_PER_1M_INPUT_TOKENS\n",
            "\n",
            "Example using 'gemini-2.5-flash' pricing ($0.08 per 1M input tokens, <128k context):\n",
            "  (10,617,293 / 1,000,000) * $0.08 = $0.8494\n",
            "\n",
            "*Note: This forecast is for INPUT tokens only. Output tokens are billed separately but are usually much smaller for classification tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-00w3pat-Cv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}