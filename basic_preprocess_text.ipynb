{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDbeIo1bw8ky",
        "outputId": "bd23f043-1ff6-4037-8ac8-97cb540a01e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted successfully.\n",
            "Source folder:      /content/drive/MyDrive/TikTok_Scraping_Output\n",
            "Destination folder: /content/drive/MyDrive/Preprocessed_Comments\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Cell 1: Mount Google Drive & Define Paths\n",
        "# =============================================================================\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Register tqdm for pandas .progress_apply()\n",
        "tqdm.pandas()\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ Google Drive mounted successfully.\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the folder where your raw batch files are stored\n",
        "SOURCE_FOLDER = '/content/drive/MyDrive/TikTok_Scraping_Output'\n",
        "\n",
        "# Define the folder where the clean text files will be saved\n",
        "DESTINATION_FOLDER = '/content/drive/MyDrive/Preprocessed_Comments'\n",
        "\n",
        "# Create the destination folder if it doesn't already exist\n",
        "os.makedirs(DESTINATION_FOLDER, exist_ok=True)\n",
        "print(f\"Source folder:      {SOURCE_FOLDER}\")\n",
        "print(f\"Destination folder: {DESTINATION_FOLDER}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 2: The Preprocessing Function\n",
        "# =============================================================================\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Applies all the required preprocessing steps to a single comment string.\n",
        "    1. Removes tagging (@username)\n",
        "    2. Converts to lowercase\n",
        "    3. Removes emojis and any characters not in the allowed set\n",
        "    4. Allowed set: Vietnamese alphabet, numbers, standard keyboard punctuation/symbols, and whitespace.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Remove tagging (@username)\n",
        "    text = re.sub(r'@[\\w\\.]+', '', text)\n",
        "\n",
        "    # 2. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Remove any character that is NOT:\n",
        "    #    - a-z (basic Latin alphabet)\n",
        "    #    - Vietnamese accented characters\n",
        "    #    - 0-9 (numbers)\n",
        "    #    - Whitespace\n",
        "    #    - Standard keyboard punctuation and symbols\n",
        "    # This comprehensive regex handles emoji removal and non-Vietnamese character removal in one step.\n",
        "    allowed_chars = r'a-zàáãạảăằắẵặẳâầấẫậẩđèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹ0-9\\s!@#$%^&*()_+\\-={}\\[\\]~`,./<>?:;\\\"\\''\n",
        "    text = re.sub(f'[^{allowed_chars}]', '', text)\n",
        "\n",
        "    # Remove extra whitespace that may have been created\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "ak0HhEFtxZdH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 3: Main Execution - Load, Process, and Save as CSV\n",
        "# =============================================================================\n",
        "\n",
        "# --- 1. Load all raw data ---\n",
        "all_batch_files = glob.glob(os.path.join(SOURCE_FOLDER, \"comments_batch_*.csv\"))\n",
        "\n",
        "if not all_batch_files:\n",
        "    print(\"❌ No batch files found in the source folder. Please check the path.\")\n",
        "else:\n",
        "    print(f\"Found {len(all_batch_files)} batch files. Loading and concatenating...\")\n",
        "    df_list = [pd.read_csv(f) for f in all_batch_files]\n",
        "    df = pd.concat(df_list, ignore_index=True)\n",
        "    initial_count = len(df)\n",
        "    print(f\"✅ Loaded a total of {initial_count} comments.\")\n",
        "\n",
        "    # --- 2. Remove duplicates based on comment_id ---\n",
        "    # This is a fast way to get rid of identical rows scraped from overlapping runs.\n",
        "    df.drop_duplicates(subset=['comment_id'], inplace=True)\n",
        "    count_after_id_dedupe = len(df)\n",
        "    print(f\"Removed {initial_count - count_after_id_dedupe} duplicate rows based on comment_id.\")\n",
        "\n",
        "    # --- 3. Preprocess the comments ---\n",
        "    print(\"\\nStarting preprocessing...\")\n",
        "    df['clean_text'] = df['comment_text'].progress_apply(preprocess_text)\n",
        "    print(\"✅ Preprocessing complete.\")\n",
        "\n",
        "    # --- 4. Remove rows where the clean_text is now empty ---\n",
        "    df.dropna(subset=['clean_text'], inplace=True)\n",
        "    df = df[df['clean_text'] != '']\n",
        "    count_after_empty_removal = len(df)\n",
        "    print(f\"Removed {count_after_id_dedupe - count_after_empty_removal} comments that became empty after cleaning.\")\n",
        "\n",
        "    # --- 5. Remove duplicates based on the clean comment content ---\n",
        "    # This keeps the first occurrence of a comment if multiple people posted the exact same thing.\n",
        "    df.drop_duplicates(subset=['clean_text'], keep='first', inplace=True)\n",
        "    final_count = len(df)\n",
        "    print(f\"Removed {count_after_empty_removal - final_count} duplicate comments based on clean_text content.\")\n",
        "    print(f\"✅ Final dataset contains {final_count} unique, non-empty comments.\")\n",
        "\n",
        "    # --- 6. Save the final DataFrame in batches ---\n",
        "    print(\"\\nSaving clean data to new CSV files...\")\n",
        "    BATCH_SIZE = 1000\n",
        "    num_files = (final_count // BATCH_SIZE) + (1 if final_count % BATCH_SIZE > 0 else 0)\n",
        "\n",
        "    for i, start_index in enumerate(range(0, final_count, BATCH_SIZE)):\n",
        "        end_index = start_index + BATCH_SIZE\n",
        "        batch_df = df.iloc[start_index:end_index]\n",
        "\n",
        "        output_filename = os.path.join(DESTINATION_FOLDER, f\"clean_comments_batch_{i+1:04d}.csv\")\n",
        "        batch_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(f\"\\n✅✅✅ All done! Saved {num_files} CSV files to '{DESTINATION_FOLDER}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-s4rlpuxfW0",
        "outputId": "66fbf048-ebb1-4cbf-a312-532116b2d2c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 batch files. Loading and concatenating...\n",
            "✅ Loaded a total of 3420 comments.\n",
            "Removed 81 duplicate rows based on comment_id.\n",
            "\n",
            "Starting preprocessing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3339/3339 [00:00<00:00, 140383.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessing complete.\n",
            "Removed 983 comments that became empty after cleaning.\n",
            "Removed 206 duplicate comments based on clean_text content.\n",
            "✅ Final dataset contains 2150 unique, non-empty comments.\n",
            "\n",
            "Saving clean data to new CSV files...\n",
            "\n",
            "✅✅✅ All done! Saved 3 CSV files to '/content/drive/MyDrive/Preprocessed_Comments'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsVG4HtZxki9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}