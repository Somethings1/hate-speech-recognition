{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Setup Complete. Libraries for API requests are installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAiXkBbNG_ud",
        "outputId": "e794af84-b4e2-4d1f-9ee3-46b64eab489d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup Complete. Libraries for API requests are installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted successfully.\")\n",
        "\n",
        "OUTPUT_FOLDER = '/content/drive/MyDrive/TikTok_Scraping_Output'\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "print(f\"All output files will be saved to: {OUTPUT_FOLDER}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB6ig4fSt0I2",
        "outputId": "b3916061-6f34-4161-d1c2-1ac80fd8cc18"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted successfully.\n",
            "All output files will be saved to: /content/drive/MyDrive/TikTok_Scraping_Output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 3: Modular Functions (Updated for Drive)\n",
        "# =============================================================================\n",
        "\n",
        "def get_processed_ids(output_folder):\n",
        "    \"\"\"Scans the specified folder in Google Drive for existing batch files.\"\"\"\n",
        "    processed_ids = set()\n",
        "    batch_files = glob.glob(os.path.join(output_folder, \"comments_batch_*.csv\"))\n",
        "    if not batch_files:\n",
        "        return processed_ids, 0\n",
        "\n",
        "    latest_batch_num = 0\n",
        "    print(f\"Found {len(batch_files)} existing batch files in Drive. Loading to resume...\")\n",
        "    for f in batch_files:\n",
        "        try:\n",
        "            batch_num = int(re.search(r'comments_batch_(\\d+).csv', f).group(1))\n",
        "            if batch_num > latest_batch_num:\n",
        "                latest_batch_num = batch_num\n",
        "\n",
        "            df = pd.read_csv(f)\n",
        "            processed_ids.update(df['video_id'].astype(str).unique())\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read or parse {f}: {e}\")\n",
        "\n",
        "    print(f\"Loaded {len(processed_ids)} already processed video IDs.\")\n",
        "    return processed_ids, latest_batch_num\n",
        "\n",
        "def fetch_comments_page(video_id, cursor=0, count=50):\n",
        "    # This function remains the same\n",
        "    API_ENDPOINT = \"https://www.tiktok.com/api/comment/list/\"\n",
        "    params = {\"aid\": \"1988\", \"aweme_id\": video_id, \"count\": count, \"cursor\": cursor}\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "    try:\n",
        "        response = requests.get(API_ENDPOINT, params=params, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException:\n",
        "        return None\n",
        "\n",
        "def extract_data_from_response(response_json, video_id):\n",
        "    # This function remains the same\n",
        "    comments_data = []\n",
        "    if not response_json or \"comments\" not in response_json or response_json[\"comments\"] is None:\n",
        "        return comments_data, 0, False\n",
        "    for comment in response_json[\"comments\"]:\n",
        "        comments_data.append({\n",
        "            'comment_id': comment.get('cid', 'N/A'),\n",
        "            'video_id': video_id,\n",
        "            'username': comment.get('user', {}).get('nickname', 'N/A'),\n",
        "            'comment_text': comment.get('text', ''),\n",
        "            'digg_count': comment.get('digg_count', 0),\n",
        "            'create_time': comment.get('create_time', 0)\n",
        "        })\n",
        "    new_cursor = response_json.get(\"cursor\", 0)\n",
        "    has_more = response_json.get(\"has_more\", 0) == 1\n",
        "    return comments_data, new_cursor, has_more\n",
        "\n",
        "def save_batch_to_csv(data, batch_num, output_folder):\n",
        "    \"\"\"Saves a batch of data to the specified folder in Google Drive.\"\"\"\n",
        "    if not data:\n",
        "        print(\"Batch is empty, nothing to save.\")\n",
        "        return\n",
        "\n",
        "    filename = f\"comments_batch_{batch_num}.csv\"\n",
        "    full_path = os.path.join(output_folder, filename)\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(full_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úÖüíæ Batch {batch_num} saved successfully with {len(data)} comments to '{full_path}'.\")"
      ],
      "metadata": {
        "id": "CWpP3cozN8cV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 4: Main Execution with Keep-Alive Loop\n",
        "# =============================================================================\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_FILE = \"link1.txt\"\n",
        "BATCH_SIZE = 20\n",
        "MAX_COMMENTS_PER_VIDEO = 100\n",
        "DELAY_BETWEEN_REQUESTS = 0.5\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "# The OUTPUT_FOLDER is defined in Cell 1\n",
        "all_comments_for_batch = []\n",
        "\n",
        "processed_ids, last_batch_num = get_processed_ids(OUTPUT_FOLDER)\n",
        "start_batch_num = last_batch_num + 1\n",
        "\n",
        "try:\n",
        "    with open(INPUT_FILE, 'r') as f:\n",
        "        all_video_ids = {line.strip() for line in f if line.strip()}\n",
        "    videos_to_process = sorted(list(all_video_ids - processed_ids))\n",
        "    if not videos_to_process:\n",
        "        print(\"‚úÖ All video IDs from the input file have already been processed.\")\n",
        "    else:\n",
        "        print(f\"Found {len(all_video_ids)} total IDs. {len(processed_ids)} already processed.\")\n",
        "        print(f\"Starting to scrape {len(videos_to_process)} new video IDs.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå ERROR: Input file not found at '{INPUT_FILE}'. Please upload it.\")\n",
        "    videos_to_process = []\n",
        "\n",
        "if videos_to_process:\n",
        "    # ... (Main scraping loop is the same)\n",
        "    batch_video_count = 0\n",
        "    current_batch_num = start_batch_num\n",
        "    for video_id in tqdm(videos_to_process, desc=\"Processing videos\"):\n",
        "        comments_for_this_video = []\n",
        "        current_cursor = 0\n",
        "        has_more_comments = True\n",
        "        while len(comments_for_this_video) < MAX_COMMENTS_PER_VIDEO and has_more_comments:\n",
        "            remaining_needed = MAX_COMMENTS_PER_VIDEO - len(comments_for_this_video)\n",
        "            count_to_fetch = min(50, remaining_needed)\n",
        "            response_data = fetch_comments_page(video_id, cursor=current_cursor, count=count_to_fetch)\n",
        "            if response_data:\n",
        "                comments_page, new_cursor, has_more_comments = extract_data_from_response(response_data, video_id)\n",
        "                if not comments_page: break\n",
        "                comments_for_this_video.extend(comments_page)\n",
        "                current_cursor = new_cursor\n",
        "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
        "            else:\n",
        "                print(f\"\\nSkipping video {video_id} due to API request failure.\")\n",
        "                break\n",
        "        all_comments_for_batch.extend(comments_for_this_video)\n",
        "        batch_video_count += 1\n",
        "        if batch_video_count >= BATCH_SIZE:\n",
        "            save_batch_to_csv(all_comments_for_batch, current_batch_num, OUTPUT_FOLDER)\n",
        "            all_comments_for_batch = []\n",
        "            batch_video_count = 0\n",
        "            current_batch_num += 1\n",
        "    if all_comments_for_batch:\n",
        "        print(\"\\nSaving final batch of leftover videos...\")\n",
        "        save_batch_to_csv(all_comments_for_batch, current_batch_num, OUTPUT_FOLDER)\n",
        "    print(f\"\\n‚úÖ‚úÖ‚úÖ All Scraping finished!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "lh2yNzlzN_UZ",
        "outputId": "3101fc67-0ee4-4bc8-822b-4a0f10d55ecb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2786 total IDs. 0 already processed.\n",
            "Starting to scrape 2786 new video IDs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing videos:   1%|          | 20/2786 [00:38<1:20:27,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖüíæ Batch 1 saved successfully with 1577 comments to '/content/drive/MyDrive/TikTok_Scraping_Output/comments_batch_1.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing videos:   1%|‚ñè         | 40/2786 [01:17<1:16:12,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖüíæ Batch 2 saved successfully with 1843 comments to '/content/drive/MyDrive/TikTok_Scraping_Output/comments_batch_2.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing videos:   2%|‚ñè         | 53/2786 [01:43<1:29:06,  1.96s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1307357296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mcomments_for_this_video\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mcurrent_cursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_cursor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDELAY_BETWEEN_REQUESTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nSkipping video {video_id} due to API request failure.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGCj1S74OY3N"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}